{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cf75988",
   "metadata": {},
   "source": [
    "## 1.Installation de gym\n",
    "\n",
    "```bash\n",
    "pip install gymnasium\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be71b50a",
   "metadata": {},
   "source": [
    "## 2. Exercices à partir de l'exemple FrozenLake\n",
    "### 2.1. Création d'un environnement Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2efc9225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# a\n",
    "import gymnasium as gym \n",
    "\n",
    "# b\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"ansi\")\n",
    "env.reset()\n",
    "\n",
    "# c\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70626380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espace d'états: Discrete(16)\n",
      "Espace d'actions: Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "# d - Visualiser les états\n",
    "print(\"Espace d'états:\", env.observation_space)\n",
    "\n",
    "# e - Visualiser les actions\n",
    "print(\"Espace d'actions:\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04de9761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilités de transition depuis l'état 0 avec l'action 2 (right):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.33333333333333337, 4, 0, False),\n",
       " (0.3333333333333333, 1, 0, False),\n",
       " (0.33333333333333337, 0, 0, False)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f - Visualiser les probabilités de transition\n",
    "# Dans gymnasium, on utilise env.unwrapped.P pour accéder aux probabilités\n",
    "# Action 2 = right, État 0 = S (état initial)\n",
    "print(\"Probabilités de transition depuis l'état 0 avec l'action 2 (right):\")\n",
    "env.unwrapped.P[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f155fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilités de transition depuis l'état 3 avec l'action 1 (down):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.33333333333333337, 2, 0, False),\n",
       " (0.3333333333333333, 7, 0, True),\n",
       " (0.33333333333333337, 3, 0, False)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# g - Probabilité de transition de l'état 3 (F) en effectuant l'action 1 (down)\n",
    "print(\"Probabilités de transition depuis l'état 3 avec l'action 1 (down):\")\n",
    "env.unwrapped.P[3][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07bd614",
   "metadata": {},
   "source": [
    "### 2.2. Générer une série d'épisodes avec Gym\n",
    "\n",
    "**Rappel:** Un épisode est l'interaction agent-environnement depuis l'état initial jusqu'à l'état terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a_reset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "État initial: 0\n"
     ]
    }
   ],
   "source": [
    "# a\n",
    "state, info = env.reset()\n",
    "print(f\"État initial: {state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b_step",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0, False, False, {'prob': 0.33333333333333337})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b\n",
    "state, info = env.reset()\n",
    "env.step(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c_random",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action séléectionnée: 1\n",
      "État suivant: 5\n",
      "Récompense: 0\n",
      "Terminé: True\n"
     ]
    }
   ],
   "source": [
    "# c\n",
    "random_action = env.action_space.sample()\n",
    "print(f\"Action séléectionnée: {random_action}\")\n",
    "\n",
    "next_state, reward, terminated, truncated, info = env.step(random_action)\n",
    "print(f\"État suivant: {next_state}\")\n",
    "print(f\"Récompense: {reward}\")\n",
    "print(f\"Terminé: {terminated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2d_episode",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "État initial: 0\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Time step 1: Action=0, État=0, Reward=0, Done=False\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Time step 2: Action=1, État=1, Reward=0, Done=False\n",
      "  (Down)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Time step 3: Action=0, État=5, Reward=0, Done=True\n",
      "  (Left)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "\n",
      "Épisode terminé au pas 3\n",
      "tombé dans un trou H\n",
      "\n",
      "État final:\n",
      "  (Left)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# d\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"ansi\")\n",
    "\n",
    "num_timesteps = 20\n",
    "\n",
    "state, info = env.reset()\n",
    "print(f\"État initial: {state}\")\n",
    "print(env.render())\n",
    "\n",
    "for t in range(num_timesteps):\n",
    "    # générer une action aléatoire\n",
    "    random_action = env.action_space.sample()\n",
    "    \n",
    "    # exécuter l'action\n",
    "    next_state, reward, terminated, truncated, info = env.step(random_action)\n",
    "    \n",
    "    # done = terminated OR truncated (pour compatibilité avec l'ancien code)\n",
    "    done = terminated or truncated\n",
    "    \n",
    "    print(f\"Time step {t+1}: Action={random_action}, État={next_state}, Reward={reward}, Done={done}\")\n",
    "    print(env.render())\n",
    "    # Sortir de la boucle si l'état terminal est atteint\n",
    "    if done:\n",
    "        print(f\"\\nÉpisode terminé au pas {t+1}\")\n",
    "        if reward == 1:\n",
    "            print(\"l'agent a atteint le but G\")\n",
    "        else:\n",
    "            print(\"tombé dans un trou H\")\n",
    "        break\n",
    "\n",
    "print(\"\\nÉtat final:\")\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2e_series",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Pas=14, Reward total=0, Echec\n",
      "Episode 2: Pas=2, Reward total=0, Echec\n",
      "Episode 3: Pas=10, Reward total=0, Echec\n",
      "Episode 4: Pas=4, Reward total=0, Echec\n",
      "Episode 5: Pas=4, Reward total=0, Echec\n",
      "Episode 6: Pas=2, Reward total=0, Echec\n",
      "Episode 7: Pas=9, Reward total=0, Echec\n",
      "Episode 8: Pas=2, Reward total=0, Echec\n",
      "Episode 9: Pas=8, Reward total=1, Succès---\n",
      "Episode 10: Pas=8, Reward total=0, Echec\n"
     ]
    }
   ],
   "source": [
    "# e\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"ansi\")\n",
    "\n",
    "num_episodes = 10\n",
    "num_timesteps = 20\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for t in range(num_timesteps):\n",
    "        random_action = env.action_space.sample()\n",
    "        next_state, reward, terminated, truncated, info = env.step(random_action)\n",
    "        total_reward += reward\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    result = \"Succès---\" if total_reward > 0 else \"Echec\"\n",
    "    print(f\"Episode {episode+1}: Pas={t+1}, Reward total={total_reward}, {result}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cartpole_intro",
   "metadata": {},
   "source": [
    "## 3. Le Cart-Pole\n",
    "\n",
    "### 3.1. Création d'un environnement Gym pour Cart-Pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3a_cartpole",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3b_state_space",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espace d'observation: Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
      "Type: Box (valeurs continues)\n",
      "\n",
      "Valeurs maximales: [4.8               inf 0.41887903        inf]\n",
      "Valeurs minimales: [-4.8               -inf -0.41887903        -inf]\n",
      "\n",
      "État initial (aléatoire):\n",
      "  Position du chariot: 0.0087\n",
      "  Vitesse du chariot: 0.0074\n",
      "  Angle de la perche: 0.0126 rad\n",
      "  Vitesse angulaire: 0.0297\n"
     ]
    }
   ],
   "source": [
    "# b - Espace d'état\n",
    "print(\"Espace d'observation:\", env.observation_space)\n",
    "print(\"Type: Box (valeurs continues)\")\n",
    "print(\"\\nValeurs maximales:\", env.observation_space.high)\n",
    "print(\"Valeurs minimales:\", env.observation_space.low)\n",
    "\n",
    "# L'état est composé de 4 valeurs:\n",
    "# [position du chariot, vitesse du chariot, angle de la perche, vitesse angulaire de la perche]\n",
    "\n",
    "# Réinitialiser et voir l'état initial\n",
    "state, info = env.reset()\n",
    "print(\"\\nÉtat initial (aléatoire):\")\n",
    "print(f\"  Position du chariot: {state[0]:.4f}\")\n",
    "print(f\"  Vitesse du chariot: {state[1]:.4f}\")\n",
    "print(f\"  Angle de la perche: {state[2]:.4f} rad\")\n",
    "print(f\"  Vitesse angulaire: {state[3]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3c_action_space",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espace d'actions: Discrete(2)\n",
      "\n",
      "Actions possibles:\n",
      "  0 = Pousser le chariot vers la gauche\n",
      "  1 = Pousser le chariot vers la droite\n"
     ]
    }
   ],
   "source": [
    "# c\n",
    "print(\"Espace d'actions:\", env.action_space)\n",
    "print(\"\\nActions possibles:\")\n",
    "print(\"  0 = Pousser le chariot vers la gauche\")\n",
    "print(\"  1 = Pousser le chariot vers la droite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cartpole_random",
   "metadata": {},
   "source": [
    "### 3.2. Équilibrage du Cart-Pole avec une politique aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3_2a_random_agent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Return: 13.0\n",
      "Episode: 10, Return: 14.0\n",
      "Episode: 20, Return: 43.0\n",
      "Episode: 30, Return: 19.0\n",
      "Episode: 40, Return: 21.0\n",
      "Episode: 50, Return: 13.0\n",
      "Episode: 60, Return: 17.0\n",
      "Episode: 70, Return: 14.0\n",
      "Episode: 80, Return: 15.0\n",
      "Episode: 90, Return: 29.0\n"
     ]
    }
   ],
   "source": [
    "# a & b\n",
    "# L'agent reçoit +1 à chaque pas où la perche reste debout\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "num_episodes = 100\n",
    "num_timesteps = 500 \n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    total_reward = 0.0\n",
    "    \n",
    "    for t in range(num_timesteps):\n",
    "        random_action = env.action_space.sample()\n",
    "        \n",
    "        next_state, reward, terminated, truncated, info = env.step(random_action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        done = terminated or truncated\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode: {episode}, Return: {total_reward}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atari_intro",
   "metadata": {},
   "source": [
    "## 4. Autres environnements Gym\n",
    "\n",
    "### 4.1 Environnements de jeux Atari\n",
    "\n",
    "Pour utiliser les environnements Atari, il faut installer les dépendances supplémentaires:\n",
    "```bash\n",
    "pip install gymnasium[atari] ale-py\n",
    "pip install gymnasium[accept-rom-license]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4_list_envs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total d'environnements disponibles: 63\n",
      "\n",
      "Quelques exemples d'environnements:\n",
      "  - CartPole-v0\n",
      "  - CartPole-v1\n",
      "  - MountainCar-v0\n",
      "  - MountainCarContinuous-v0\n",
      "  - Pendulum-v1\n",
      "  - Acrobot-v1\n",
      "  - phys2d/CartPole-v0\n",
      "  - phys2d/CartPole-v1\n",
      "  - phys2d/Pendulum-v0\n",
      "  - LunarLander-v3\n",
      "  - LunarLanderContinuous-v3\n",
      "  - BipedalWalker-v3\n",
      "  - BipedalWalkerHardcore-v3\n",
      "  - CarRacing-v3\n",
      "  - Blackjack-v1\n",
      "  - FrozenLake-v1\n",
      "  - FrozenLake8x8-v1\n",
      "  - CliffWalking-v1\n",
      "  - CliffWalkingSlippery-v1\n",
      "  - Taxi-v3\n"
     ]
    }
   ],
   "source": [
    "all_envs = list(gym.envs.registry.keys())\n",
    "print(f\"Nombre total d'environnements disponibles: {len(all_envs)}\")\n",
    "print(\"\\nQuelques exemples d'environnements:\")\n",
    "for env_name in all_envs[:20]:\n",
    "    print(f\"  - {env_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a_tennis_space",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espace d'observation (écran du jeu):\n",
      "  Shape: (210, 160, 3)\n",
      "  Type: uint8\n",
      "\n",
      "Espace d'actions:\n",
      "  Nombre d'actions: 18\n"
     ]
    }
   ],
   "source": [
    "# a : Tennis\n",
    "import ale_py\n",
    "import gymnasium as gym\n",
    "\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "env = gym.make(\"ALE/Tennis-v5\", render_mode=\"rgb_array\")\n",
    "\n",
    "print(\"Espace d'observation (écran du jeu):\")\n",
    "print(f\"  Shape: {env.observation_space.shape}\")\n",
    "print(f\"  Type: {env.observation_space.dtype}\")\n",
    "\n",
    "print(\"\\nEspace d'actions:\")\n",
    "print(f\"  Nombre d'actions: {env.action_space.n}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b_tennis_agent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Return: -1.0\n",
      "Episode: 10, Return: -1.0\n",
      "Episode: 20, Return: -1.0\n",
      "Episode: 30, Return: -1.0\n",
      "Episode: 40, Return: 0.0\n",
      "Episode: 50, Return: -1.0\n",
      "Episode: 60, Return: -1.0\n",
      "Episode: 70, Return: -1.0\n",
      "Episode: 80, Return: 0.0\n",
      "Episode: 90, Return: -1.0\n"
     ]
    }
   ],
   "source": [
    "# b - Agent jouant au jeu Tennis avec politique aléatoire\n",
    "\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "env = gym.make(\"ALE/Tennis-v5\", render_mode=\"rgb_array\")\n",
    "\n",
    "num_episodes = 100\n",
    "num_timesteps = 50\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    total_reward = 0.0\n",
    "    \n",
    "    for t in range(num_timesteps):\n",
    "        random_action = env.action_space.sample()\n",
    "        next_state, reward, terminated, truncated, info = env.step(random_action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        done = terminated or truncated\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode: {episode}, Return: {total_reward}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c_record_video",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Return: -6.0\n",
      "Episode: 1, Return: -8.0\n",
      "Episode: 2, Return: -7.0\n",
      "Episode: 3, Return: -7.0\n",
      "Episode: 4, Return: -8.0\n",
      "\n",
      "Vidéos enregistrées dans: ./recording\n"
     ]
    }
   ],
   "source": [
    "# c\n",
    "\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import os\n",
    "\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "recording_dir = \"./recording\"\n",
    "os.makedirs(recording_dir, exist_ok=True)\n",
    "\n",
    "env = gym.make(\"ALE/Tennis-v5\", render_mode=\"rgb_array\")\n",
    "\n",
    "env = RecordVideo(\n",
    "    env, \n",
    "    video_folder=recording_dir,\n",
    "    episode_trigger=lambda episode_id: episode_id % 10 == 0,  # Enregistrer tous les 10 épisodes\n",
    "    name_prefix=\"tennis\"\n",
    ")\n",
    "\n",
    "num_episodes = 5\n",
    "num_timesteps = 500\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    total_reward = 0.0\n",
    "    \n",
    "    for t in range(num_timesteps):\n",
    "        random_action = env.action_space.sample()\n",
    "        next_state, reward, terminated, truncated, info = env.step(random_action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        done = terminated or truncated\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    print(f\"Episode: {episode}, Return: {total_reward}\")\n",
    "\n",
    "env.close()\n",
    "print(f\"\\nVidéos enregistrées dans: {recording_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4_2_other",
   "metadata": {},
   "source": [
    "### 4.2 Autres Environnements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "explore_envs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MountainCar:\n",
      "  Espace d'observation: Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
      "  Espace d'actions: Discrete(3)\n",
      "\n",
      "Acrobot:\n",
      "  Espace d'observation: Box([ -1.        -1.        -1.        -1.       -12.566371 -28.274334], [ 1.        1.        1.        1.       12.566371 28.274334], (6,), float32)\n",
      "  Espace d'actions: Discrete(3)\n",
      "\n",
      "Pendulum:\n",
      "  Espace d'observation: Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)\n",
      "  Espace d'actions: Box(-2.0, 2.0, (1,), float32)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\")\n",
    "print(\"MountainCar:\")\n",
    "print(f\"  Espace d'observation: {env.observation_space}\")\n",
    "print(f\"  Espace d'actions: {env.action_space}\")\n",
    "env.close()\n",
    "\n",
    "env = gym.make(\"Acrobot-v1\", render_mode=\"rgb_array\")\n",
    "print(\"\\nAcrobot:\")\n",
    "print(f\"  Espace d'observation: {env.observation_space}\")\n",
    "print(f\"  Espace d'actions: {env.action_space}\")\n",
    "env.close()\n",
    "\n",
    "env = gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\")\n",
    "print(\"\\nPendulum:\")\n",
    "print(f\"  Espace d'observation: {env.observation_space}\")\n",
    "print(f\"  Espace d'actions: {env.action_space}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7272d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
